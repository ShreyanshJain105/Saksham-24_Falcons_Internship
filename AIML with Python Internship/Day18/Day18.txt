#============Theory session  ===================#
Feature engineeering =. 

datacleaning

column => car  => mieae ,tor,qualitty,model,prize
row format

child=>
cow =. white ,big 

machine => 


missing value => NAN || Transform 


mode ,median , std 

home = romms[baths,dinin, bad, washroom ,balcony]  x
1bhk,2bbhk,square fir , area ,fecult (gather)

Feature extraction 
goal = prce find out 



// imporant points of feature engineeering

Foundation for Success: Feature engineering is the cornerstone of building effective machine learning models. "Garbage in, garbage out" applies â€“ the quality of your features directly impacts model performance.

Data Transformation: Raw data often needs cleaning, normalization, and handling of missing values before feeding it to a model. Feature engineering tackles these preprocessing steps.

Feature Selection: Not all data points are equally important. Choosing the most relevant features that best represent the problem helps models learn more efficiently.

Feature Creation: Deriving new features from existing ones can unlock hidden patterns and relationships within the data. This can significantly boost model performance.

Understanding the Problem: Feature engineering choices heavily depend on the specific machine learning task at hand.  A deep understanding of the problem is crucial.

Domain Knowledge is Key:  Incorporating knowledge about the data domain helps identify the most informative features and avoid irrelevant ones.

Iterative Process: Feature engineering is rarely a one-shot deal. As you develop and test your model, revisit and refine your feature choices based on results.

Bias Awareness: Be cautious of introducing bias during feature engineering. Unequal representation in data or feature selection can skew model predictions.

Feature Scaling: Standardizing features to a common scale ensures all features contribute equally during model training, especially for algorithms sensitive to scale.

Dimensionality Reduction: Complex datasets with many features can be overwhelming for models. Feature engineering techniques like Principal Component Analysis (PCA) can reduce dimensionality while preserving key information.
